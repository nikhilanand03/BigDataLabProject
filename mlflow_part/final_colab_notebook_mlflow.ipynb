{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FBh_lRSgYNLo"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install mlflow\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import subprocess\n",
        "from pyngrok import ngrok, conf\n",
        "import getpass"
      ],
      "metadata": {
        "id": "FRs21_URYN_A"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MLFLOW_TRACKING_URI = \"sqlite:///mlflow.db\"\n",
        "subprocess.Popen([\"mlflow\", \"ui\", \"--backend-store-uri\", MLFLOW_TRACKING_URI])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIZdVDyCYRjn",
        "outputId": "418b2133-4ff9-4810-cb93-fd5d141c0632"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['mlflow', 'ui', '--backend-store-uri', 'sqli...>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
        "# mlflow will create an experiment if it doesn't exist\n",
        "mlflow.set_experiment(\"duration-prediction-experiment\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OnygshxcYZCl",
        "outputId": "b68b3ac3-b1e1-43cc-cec5-f18a44ec6433"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='/content/mlruns/1', creation_time=1715896345845, experiment_id='1', last_update_time=1715896345845, lifecycle_stage='active', name='duration-prediction-experiment', tags={}>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\")\n",
        "conf.get_default().auth_token = getpass.getpass()\n",
        "port=5000\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(f' * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqNtqjd6Yabs",
        "outputId": "a3c11e2a-a808-4c29-8310-de4b9d371ad0"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your authtoken, which can be copied from https://dashboard.ngrok.com/auth\n",
            "··········\n",
            " * ngrok tunnel \"https://7112-35-202-246-121.ngrok-free.app\" -> \"http://127.0.0.1:5000\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.set_experiment(\"BDL Project\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mCcxVatxYcuq",
        "outputId": "6f8fcff5-f807-4c3b-d84c-e0dbf2ed19a4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='/content/mlruns/2', creation_time=1715896355333, experiment_id='2', last_update_time=1715896355333, lifecycle_stage='active', name='BDL Project', tags={}>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install efficientnet-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpl_rX-5Yjbe",
        "outputId": "c2c6f27e-0bbc-44f6-88d6-14d32d5596d9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting efficientnet-pytorch\n",
            "  Downloading efficientnet_pytorch-0.7.1.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from efficientnet-pytorch) (2.2.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->efficientnet-pytorch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->efficientnet-pytorch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->efficientnet-pytorch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->efficientnet-pytorch) (1.3.0)\n",
            "Building wheels for collected packages: efficientnet-pytorch\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.1-py3-none-any.whl size=16429 sha256=fb128eb950c4c6b1448e0aa9764992af7027fc43f34af12d4976c0f834fd69ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/3f/e9/911b1bc46869644912bda90a56bcf7b960f20b5187feea3baf\n",
            "Successfully built efficientnet-pytorch\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, efficientnet-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.7.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "from torch import nn,optim\n",
        "import os\n",
        "import mlflow\n",
        "from mlflow.models import infer_signature\n",
        "\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI8pZx0zYlML",
        "outputId": "b20a9bd7-977e-4d05-88a4-cbfc7380526c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "def load_checkpoint(checkpoint,model,optimizer,lr):\n",
        "    print(\"Loading checkpoint...\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group[\"lr\"] = lr\n",
        "\n",
        "def save_checkpoint(state,filename='checkpt.pth.tar'):\n",
        "    print(\"Saving checkpoint...\")\n",
        "    torch.save(state,filename)\n",
        "\n",
        "# def get_submission(loader, dataset, model):\n",
        "#     model.eval()\n",
        "#     predictions = pd.DataFrame()\n",
        "#     image_id = 1\n",
        "\n",
        "#     for image, label in tqdm(loader):\n",
        "#         image = image.to(myconfig.DEVICE)\n",
        "#         preds = torch.clip(model(image).squeeze(0), 0.0, 96.0)\n",
        "#         predictions = pd.concat([predictions,pd.Series(preds.detach().numpy()).to_frame().T],ignore_index=True)\n",
        "\n",
        "#         image_id += 1\n",
        "\n",
        "#     predictions.to_csv(\"submission.csv\", index=False)\n",
        "#     model.train()\n",
        "\n",
        "def get_rmse(loader, model, loss_fn, device):\n",
        "    model.eval()\n",
        "    num_examples = 0\n",
        "    losses = []\n",
        "    for batch_idx, (data, targets) in enumerate(loader):\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        scores = model(data)\n",
        "        loss = loss_fn(scores[targets != -1], targets[targets != -1])\n",
        "        num_examples += scores[targets != -1].shape[0]\n",
        "        losses.append(loss.item())\n",
        "\n",
        "    model.train()\n",
        "    print(f\"Loss on val: {(sum(losses)/num_examples)**0.5}\")\n",
        "    return (sum(losses)/num_examples)**0.5"
      ],
      "metadata": {
        "id": "cMHhqvx4YqCd"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader,Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class FacialKeypointDataset(Dataset):\n",
        "    def __init__(self, csv_file, train=True, transform=None):\n",
        "        super().__init__()\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.category_names = ['left_eye_center_x', 'left_eye_center_y', 'right_eye_center_x', 'right_eye_center_y', 'left_eye_inner_corner_x', 'left_eye_inner_corner_y', 'left_eye_outer_corner_x', 'left_eye_outer_corner_y', 'right_eye_inner_corner_x', 'right_eye_inner_corner_y', 'right_eye_outer_corner_x', 'right_eye_outer_corner_y', 'left_eyebrow_inner_end_x', 'left_eyebrow_inner_end_y', 'left_eyebrow_outer_end_x', 'left_eyebrow_outer_end_y', 'right_eyebrow_inner_end_x', 'right_eyebrow_inner_end_y', 'right_eyebrow_outer_end_x', 'right_eyebrow_outer_end_y', 'nose_tip_x', 'nose_tip_y', 'mouth_left_corner_x', 'mouth_left_corner_y', 'mouth_right_corner_x', 'mouth_right_corner_y', 'mouth_center_top_lip_x', 'mouth_center_top_lip_y', 'mouth_center_bottom_lip_x', 'mouth_center_bottom_lip_y']\n",
        "        self.transform = transform\n",
        "        self.train = train\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if self.train:\n",
        "            image = np.array(self.data.iloc[index, 30].split()).astype(np.float32)\n",
        "            labels = np.array(self.data.iloc[index, :30].tolist())\n",
        "            labels[np.isnan(labels)] = -1\n",
        "        else:\n",
        "            image = np.array(self.data.iloc[index, 0].split()).astype(np.float32)\n",
        "            labels = np.zeros(30)\n",
        "\n",
        "        ignore_indices = labels == -1\n",
        "        labels = labels.reshape(15, 2)\n",
        "\n",
        "        if self.transform:\n",
        "            image = np.repeat(image.reshape(96, 96, 1), 3, 2).astype(np.uint8)\n",
        "            augmentations = self.transform(image=image, keypoints=labels)\n",
        "            image = augmentations[\"image\"]\n",
        "            labels = augmentations[\"keypoints\"]\n",
        "\n",
        "        labels = np.array(labels).reshape(-1)\n",
        "        labels[ignore_indices] = -1\n",
        "\n",
        "        return image, labels.astype(np.float32)"
      ],
      "metadata": {
        "id": "VJJfcR5hYrWW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import cv2\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "LEARNING_RATE = 8e-6\n",
        "WEIGHT_DECAY = 1e-3\n",
        "BATCH_SIZE = 64\n",
        "NUM_EPOCHS = 100\n",
        "NUM_WORKERS = 4\n",
        "CHECKPOINT_FILE = \"b0.pth.tar\"\n",
        "PIN_MEMORY = True\n",
        "SAVE_MODEL = True\n",
        "LOAD_MODEL = True\n",
        "\n",
        "train_transforms = A.Compose(\n",
        "    [\n",
        "        A.Resize(width=96, height=96),\n",
        "        A.Rotate(limit=15, border_mode=cv2.BORDER_CONSTANT, p=0.8),\n",
        "        A.HorizontalFlip(p=0.4),\n",
        "        A.RandomBrightnessContrast(contrast_limit=0.5, brightness_limit=0.5, p=0.2),\n",
        "        A.OneOf([\n",
        "            A.GaussNoise(p=0.8),\n",
        "            A.CLAHE(p=0.8),\n",
        "            A.ImageCompression(p=0.8),\n",
        "            A.RandomGamma(p=0.8),\n",
        "            A.Posterize(p=0.8),\n",
        "            A.Blur(p=0.8),\n",
        "        ], p=1.0),\n",
        "        A.OneOf([\n",
        "            A.GaussNoise(p=0.8),\n",
        "            A.CLAHE(p=0.8),\n",
        "            A.ImageCompression(p=0.8),\n",
        "            A.RandomGamma(p=0.8),\n",
        "            A.Posterize(p=0.8),\n",
        "            A.Blur(p=0.8),\n",
        "        ], p=1.0),\n",
        "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=0, p=0.2, border_mode=cv2.BORDER_CONSTANT),\n",
        "        A.Normalize(mean=[0.4897, 0.4897, 0.4897], std=[0.2330, 0.2330, 0.2330], max_pixel_value=255.0,\n",
        "        ),\n",
        "        ToTensorV2(),\n",
        "    ], keypoint_params=A.KeypointParams(format=\"xy\", remove_invisible=False),\n",
        ")\n",
        "\n",
        "\n",
        "val_transforms = A.Compose(\n",
        "    [\n",
        "        A.Resize(height=96, width=96),\n",
        "        A.Normalize(mean=[0.4897, 0.4897, 0.4897], std=[0.2330, 0.2330, 0.2330], max_pixel_value=255.0),\n",
        "        ToTensorV2(),\n",
        "    ], keypoint_params=A.KeypointParams(format=\"xy\", remove_invisible=False),\n",
        ")"
      ],
      "metadata": {
        "id": "E7cSV6TXYss8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(loader, model, optimizer, loss_fn, scaler, device):\n",
        "    losses = []\n",
        "    loop = tqdm(loader)\n",
        "    num_examples = 0\n",
        "    for batch_idx, (data, targets) in enumerate(loop):\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        scores = model(data)\n",
        "        scores[targets == -1] = -1\n",
        "        loss = loss_fn(scores, targets)\n",
        "        num_examples += torch.numel(scores[targets != -1])\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Loss average over epoch: {(sum(losses)/num_examples)**0.5}\")"
      ],
      "metadata": {
        "id": "sNoiTd3DYuQy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXxXIdrAYvnU",
        "outputId": "874ff269-d4d6-4b67-9728-35471a5aad91"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = FacialKeypointDataset(\n",
        "    csv_file=\"/content/gdrive/MyDrive/Colab Notebooks/training_new.csv\",\n",
        "    transform=train_transforms\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        "    shuffle=True\n",
        ")\n",
        "val_ds = FacialKeypointDataset(\n",
        "    transform=val_transforms,\n",
        "    csv_file=\"/content/gdrive/MyDrive/Colab Notebooks/val_new.csv\"\n",
        ")\n",
        "val_loader=DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=PIN_MEMORY,\n",
        "    shuffle=False\n",
        ")\n",
        "# test_ds = FacialKeypointDataset(\n",
        "#     csv_file=\"/Users/nikhilanand/JupyterNotebooks/InterIIT2023/CVSelections/test.csv\",\n",
        "#     transform=val_transforms,\n",
        "#     train=False,\n",
        "# )\n",
        "\n",
        "# test_loader = DataLoader(\n",
        "#     test_ds,\n",
        "#     batch_size=1,\n",
        "#     num_workers=NUM_WORKERS,\n",
        "#     pin_memory=PIN_MEMORY,\n",
        "#     shuffle=False,\n",
        "# )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viGO8yLlY1lI",
        "outputId": "02992dc1-20b2-4f77-a76b-8161a4e0d179"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 8e-5\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "with mlflow.start_run(run_name=\"trial2\"):\n",
        "\n",
        "    loss_fn = nn.MSELoss(reduction=\"sum\")\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    model._fc = nn.Linear(1280, 30)\n",
        "    model = model.to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    if LOAD_MODEL and CHECKPOINT_FILE in os.listdir():\n",
        "        load_checkpoint(torch.load(CHECKPOINT_FILE), model, optimizer, LEARNING_RATE)\n",
        "\n",
        "    params = {\n",
        "        \"model\": model,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"weight_decay\": WEIGHT_DECAY,\n",
        "        \"num_epochs\":10\n",
        "    }\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        print(\"Epoch \",epoch)\n",
        "        get_rmse(val_loader, model, loss_fn, DEVICE)\n",
        "        train_one_epoch(train_loader, model, optimizer, loss_fn, scaler, DEVICE)\n",
        "\n",
        "        if SAVE_MODEL:\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "            save_checkpoint(checkpoint, filename=\"b1.pth.tar\")\n",
        "\n",
        "    rmse = get_rmse(val_loader, model, loss_fn, DEVICE)\n",
        "    # Log the loss metric\n",
        "    mlflow.log_metric(\"rmse_loss\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4N4neVcZ1Hg",
        "outputId": "05bbbb3d-eb77-4ca1-8358-c8e0ca5ead22"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024/05/16 21:57:00 WARNING mlflow.utils.validation: Param value 'EfficientNet(\n",
            "  (_conv_stem): Conv2dStaticSamePadding(\n",
            "    3, 32, kernel_size=(3, 3), stride=(2, 2),...' (14651 characters) is truncated to 6000 characters to meet the length limit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n",
            "Epoch  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on val: 51.993571304554024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:56<00:00,  1.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 52.177165239145424\n",
            "Saving checkpoint...\n",
            "Epoch  1\n",
            "Loss on val: 49.017125975242045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:58<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 49.050640185495176\n",
            "Saving checkpoint...\n",
            "Epoch  2\n",
            "Loss on val: 37.20966510299818\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:51<00:00,  1.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 45.142964355041556\n",
            "Saving checkpoint...\n",
            "Epoch  3\n",
            "Loss on val: 34.839114251476396\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:54<00:00,  1.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 41.960121831878304\n",
            "Saving checkpoint...\n",
            "Epoch  4\n",
            "Loss on val: 31.406737092626255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:57<00:00,  1.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 38.50914958697213\n",
            "Saving checkpoint...\n",
            "Epoch  5\n",
            "Loss on val: 23.323405567718726\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:51<00:00,  1.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 33.971181306465866\n",
            "Saving checkpoint...\n",
            "Epoch  6\n",
            "Loss on val: 20.726881388419923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:55<00:00,  1.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 29.574870199094477\n",
            "Saving checkpoint...\n",
            "Epoch  7\n",
            "Loss on val: 19.081702078881577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:52<00:00,  1.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 25.96631550435859\n",
            "Saving checkpoint...\n",
            "Epoch  8\n",
            "Loss on val: 18.973726133116067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:57<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 23.157715383136342\n",
            "Saving checkpoint...\n",
            "Epoch  9\n",
            "Loss on val: 18.602223559203903\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:55<00:00,  1.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 20.790774215229984\n",
            "Saving checkpoint...\n",
            "Loss on val: 18.150844707226472\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 8e-5\n",
        "WEIGHT_DECAY = 1e-4\n",
        "\n",
        "with mlflow.start_run(run_name=\"trial3\"):\n",
        "\n",
        "    loss_fn = nn.MSELoss(reduction=\"sum\")\n",
        "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
        "    model._fc = nn.Linear(1280, 30)\n",
        "    model = model.to(DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "    if LOAD_MODEL and CHECKPOINT_FILE in os.listdir():\n",
        "        load_checkpoint(torch.load(CHECKPOINT_FILE), model, optimizer, LEARNING_RATE)\n",
        "\n",
        "    params = {\n",
        "        \"model\": model,\n",
        "        \"learning_rate\": LEARNING_RATE,\n",
        "        \"weight_decay\": WEIGHT_DECAY,\n",
        "        \"num_epochs\":10\n",
        "    }\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "    for epoch in range(10):\n",
        "        if(epoch>=7):\n",
        "          LEARNING_RATE = 1e-5\n",
        "          optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "        print(\"Epoch \",epoch)\n",
        "        get_rmse(val_loader, model, loss_fn, DEVICE)\n",
        "        train_one_epoch(train_loader, model, optimizer, loss_fn, scaler, DEVICE)\n",
        "\n",
        "        if SAVE_MODEL:\n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "            save_checkpoint(checkpoint, filename=\"b1.pth.tar\")\n",
        "\n",
        "    rmse = get_rmse(val_loader, model, loss_fn, DEVICE)\n",
        "    # Log the loss metric\n",
        "    mlflow.log_metric(\"rmse_loss\", rmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0uU1VQoZ55E",
        "outputId": "0435207e-b36a-4f51-86d3-d8488d812440"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024/05/16 22:11:33 WARNING mlflow.utils.validation: Param value 'EfficientNet(\n",
            "  (_conv_stem): Conv2dStaticSamePadding(\n",
            "    3, 32, kernel_size=(3, 3), stride=(2, 2),...' (14651 characters) is truncated to 6000 characters to meet the length limit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained weights for efficientnet-b0\n",
            "Epoch  0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on val: 52.02158735831046\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:56<00:00,  1.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 52.149076874336515\n",
            "Saving checkpoint...\n",
            "Epoch  1\n",
            "Loss on val: 48.52466003450478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:55<00:00,  1.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 48.739591723689834\n",
            "Saving checkpoint...\n",
            "Epoch  2\n",
            "Loss on val: 40.666075533578734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:58<00:00,  1.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 45.19311609120866\n",
            "Saving checkpoint...\n",
            "Epoch  3\n",
            "Loss on val: 34.41535175854999\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:55<00:00,  1.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 42.255248334200104\n",
            "Saving checkpoint...\n",
            "Epoch  4\n",
            "Loss on val: 28.580381907061973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:53<00:00,  1.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 38.867946120983866\n",
            "Saving checkpoint...\n",
            "Epoch  5\n",
            "Loss on val: 21.955090940657062\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [01:04<00:00,  1.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 34.19857240045058\n",
            "Saving checkpoint...\n",
            "Epoch  6\n",
            "Loss on val: 19.23831741098052\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:53<00:00,  1.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 29.781675494694365\n",
            "Saving checkpoint...\n",
            "Epoch  7\n",
            "Loss on val: 19.15997294145724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:55<00:00,  1.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 27.453023814470797\n",
            "Saving checkpoint...\n",
            "Epoch  8\n",
            "Loss on val: 22.029197594284373\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:59<00:00,  1.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 26.954230657256062\n",
            "Saving checkpoint...\n",
            "Epoch  9\n",
            "Loss on val: 23.60144120486168\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84/84 [00:54<00:00,  1.54it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss average over epoch: 26.547189677937865\n",
            "Saving checkpoint...\n",
            "Loss on val: 24.015826707653908\n"
          ]
        }
      ]
    }
  ]
}