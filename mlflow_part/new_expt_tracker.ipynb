{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nikhilanand/BigDataLabProject/mlflow_part\n"
     ]
    }
   ],
   "source": [
    "from mydatasetclass import FacialKeypointDataset\n",
    "import myconfig\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "from torch import nn,optim\n",
    "import os\n",
    "from myfuncs import load_checkpoint,get_rmse,get_submission,save_checkpoint\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/17 01:30:19 INFO mlflow.tracking.fluent: Experiment with name 'BDL Project' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/698340650722622115', creation_time=1715889619794, experiment_id='698340650722622115', last_update_time=1715889619794, lifecycle_stage='active', name='BDL Project', tags={}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(uri=\"http://127.0.0.1:8080\")\n",
    "mlflow.set_experiment(\"BDL Project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = FacialKeypointDataset(\n",
    "    csv_file=\"/Users/nikhilanand/BigDataLabProject/data/training_new.csv\",\n",
    "    transform=myconfig.train_transforms\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=myconfig.BATCH_SIZE,\n",
    "    num_workers=myconfig.NUM_WORKERS,\n",
    "    pin_memory=myconfig.PIN_MEMORY,\n",
    "    shuffle=True\n",
    ")\n",
    "val_ds = FacialKeypointDataset(\n",
    "    transform=myconfig.val_transforms,\n",
    "    csv_file=\"/Users/nikhilanand/BigDataLabProject/data/val_new.csv\"\n",
    ")\n",
    "val_loader=DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=myconfig.BATCH_SIZE,\n",
    "    num_workers=myconfig.NUM_WORKERS,\n",
    "    pin_memory=myconfig.PIN_MEMORY,\n",
    "    shuffle=False\n",
    ")\n",
    "# test_ds = FacialKeypointDataset(\n",
    "#     csv_file=\"/Users/nikhilanand/JupyterNotebooks/InterIIT2023/CVSelections/test.csv\",\n",
    "#     transform=myconfig.val_transforms,\n",
    "#     train=False,\n",
    "# )\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#     test_ds,\n",
    "#     batch_size=1,\n",
    "#     num_workers=myconfig.NUM_WORKERS,\n",
    "#     pin_memory=myconfig.PIN_MEMORY,\n",
    "#     shuffle=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll run three expts. Will take an hour to run all three.\n",
    "- First I'll try with lr of 8e-6, w_d of 1e-3.\n",
    "- Then I'll try with lr of 8e-5, w_d of 1e-5.\n",
    "- Then I'll try with lr of 8e-4, w_d of 1e-6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loading checkpoint...\n",
      "Epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bioinfo/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on val: 51.992409905935745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 84/84 [36:16<00:00, 25.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss average over epoch: 52.63855821567224\n",
      "Saving checkpoint...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on val: 51.86037865462463\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 8e-6\n",
    "WEIGHT_DECAY = 1e-3\n",
    "\n",
    "with mlflow.start_run(run_name=\"trial1\"):\n",
    "\n",
    "    loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
    "    model._fc = nn.Linear(1280, 30)\n",
    "    model = model.to(myconfig.DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    if myconfig.LOAD_MODEL and myconfig.CHECKPOINT_FILE in os.listdir():\n",
    "        load_checkpoint(torch.load(myconfig.CHECKPOINT_FILE), model, optimizer, LEARNING_RATE)\n",
    "\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"num_epochs\":1\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        print(\"Epoch \",epoch)\n",
    "        get_rmse(val_loader, model, loss_fn, myconfig.DEVICE)\n",
    "        train_one_epoch(train_loader, model, optimizer, loss_fn, scaler, myconfig.DEVICE)\n",
    "\n",
    "        if myconfig.SAVE_MODEL:\n",
    "            checkpoint = {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "            }\n",
    "            save_checkpoint(checkpoint, filename=\"b0.pth.tar\")\n",
    "               \n",
    "    rmse = get_rmse(val_loader, model, loss_fn, myconfig.DEVICE)\n",
    "    # Log the loss metric\n",
    "    mlflow.log_metric(\"rmse_loss\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/05/16 20:38:34 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loading checkpoint...\n",
      "Epoch  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/bioinfo/lib/python3.12/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on val: 51.992409905935745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/84 [01:06<45:08, 33.04s/it]"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 8e-5\n",
    "WEIGHT_DECAY = 1e-5\n",
    "\n",
    "with mlflow.start_run(run_name=\"trial2\"):\n",
    "    # mlflow.autolog()\n",
    "    loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
    "    model._fc = nn.Linear(1280, 30)\n",
    "    model = model.to(myconfig.DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    if myconfig.LOAD_MODEL and myconfig.CHECKPOINT_FILE in os.listdir():\n",
    "        load_checkpoint(torch.load(myconfig.CHECKPOINT_FILE), model, optimizer, LEARNING_RATE)\n",
    "\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"num_epochs\":1\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    for epoch in range(1):\n",
    "        print(\"Epoch \",epoch)\n",
    "        get_rmse(val_loader, model, loss_fn, myconfig.DEVICE)\n",
    "        train_one_epoch(train_loader, model, optimizer, loss_fn, scaler, myconfig.DEVICE)\n",
    "\n",
    "        if myconfig.SAVE_MODEL:\n",
    "            checkpoint = {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "            }\n",
    "            save_checkpoint(checkpoint, filename=\"b1.pth.tar\")\n",
    "            \n",
    "    rmse = get_rmse(val_loader, model, loss_fn, myconfig.DEVICE)\n",
    "    mlflow.log_metric(\"rmse_loss\", rmse)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 8e-4\n",
    "WEIGHT_DECAY = 1e-6\n",
    "\n",
    "with mlflow.start_run(run_name=\"trial3\"):\n",
    "    mlflow.autolog()\n",
    "    loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "    model = EfficientNet.from_pretrained(\"efficientnet-b0\")\n",
    "    model._fc = nn.Linear(1280, 30)\n",
    "    model = model.to(myconfig.DEVICE)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "    if myconfig.LOAD_MODEL and myconfig.CHECKPOINT_FILE in os.listdir():\n",
    "        load_checkpoint(torch.load(myconfig.CHECKPOINT_FILE), model, optimizer, LEARNING_RATE)\n",
    "\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"num_epochs\":1\n",
    "    }\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    for epoch in range(1):\n",
    "        print(\"Epoch \",epoch)\n",
    "        get_rmse(val_loader, model, loss_fn, myconfig.DEVICE)\n",
    "        train_one_epoch(train_loader, model, optimizer, loss_fn, scaler, myconfig.DEVICE)\n",
    "\n",
    "        if myconfig.SAVE_MODEL:\n",
    "            checkpoint = {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "            }\n",
    "            save_checkpoint(checkpoint, filename=\"b2.pth.tar\")\n",
    "            \n",
    "    rmse = get_rmse(val_loader, model, loss_fn, myconfig.DEVICE)\n",
    "    mlflow.log_metric(\"rmse_loss\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioinfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
