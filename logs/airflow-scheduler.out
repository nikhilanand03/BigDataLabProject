[2024-05-16 12:53:40 +0530] [86627] [INFO] Starting gunicorn 20.1.0
[2024-05-16 12:53:40 +0530] [86627] [ERROR] Connection in use: ('0.0.0.0', 8793)
[2024-05-16 12:53:40 +0530] [86627] [ERROR] Retrying in 1 second.
[2024-05-16 12:53:41 +0530] [86627] [ERROR] Connection in use: ('0.0.0.0', 8793)
[2024-05-16 12:53:41 +0530] [86627] [ERROR] Retrying in 1 second.
[2024-05-16 12:53:42 +0530] [86627] [ERROR] Connection in use: ('0.0.0.0', 8793)
[2024-05-16 12:53:42 +0530] [86627] [ERROR] Retrying in 1 second.
[2024-05-16 12:53:43 +0530] [86627] [ERROR] Connection in use: ('0.0.0.0', 8793)
[2024-05-16 12:53:43 +0530] [86627] [ERROR] Retrying in 1 second.
[2024-05-16 12:53:44 +0530] [86627] [ERROR] Connection in use: ('0.0.0.0', 8793)
[2024-05-16 12:53:44 +0530][[34m2024-05-16 12:58:39,849[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 12:59:32,081[0m] {[34mdag.py:[0m2935} INFO[0m - Setting next_dagrun for keypoints_dag to 2024-05-16T00:00:00+00:00[0m
[[34m2024-05-16 12:59:32,105[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 2 tasks up for execution:
	<TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: keypoints_dag.preprocess_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-16 12:59:32,106[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued[0m
[[34m2024-05-16 12:59:32,106[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-16 12:59:32,107[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 1/16 running and queued tasks[0m
[[34m2024-05-16 12:59:32,107[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: keypoints_dag.preprocess_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-16 12:59:32,108[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='fetch_data', run_id='scheduled__2024-05-15T00:00:00+00:00', try_number=1) to executor with priority 1 and queue default[0m
[[34m2024-05-16 12:59:32,108[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'scheduled__2024-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 12:59:32,109[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='preprocess_data', run_id='scheduled__2024-05-15T00:00:00+00:00', try_number=1) to executor with priority 1 and queue default[0m
[[34m2024-05-16 12:59:32,109[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'scheduled__2024-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 12:59:32,110[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'scheduled__2024-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 12:59:32,892[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BigDataLabProject/dags/preprocess_airflow.py[0m
[[34m2024-05-16 12:59:33,273[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-16 12:59:33,282[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-16 12:59:33,295[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-15T00:00:00+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-16 12:59:34,209[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'scheduled__2024-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 12:59:35,019[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BigDataLabProject/dags/preprocess_airflow.py[0m
[[34m2024-05-16 12:59:35,393[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-16 12:59:35,401[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-16 12:59:35,413[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.preprocess_data scheduled__2024-05-15T00:00:00+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-16 12:59:35,742[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.fetch_data run_id=scheduled__2024-05-15T00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2024-05-16 12:59:35,743[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.preprocess_data run_id=scheduled__2024-05-15T00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2024-05-16 12:59:35,748[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=fetch_data, run_id=scheduled__2024-05-15T00:00:00+00:00, run_start_date=2024-05-16 07:29:33.343244+00:00, run_end_date=2024-05-16 07:29:34.034137+00:00, run_duration=0.690893, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=2, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator[0m
[[34m2024-05-16 12:59:35,748[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=preprocess_data, run_id=scheduled__2024-05-15T00:00:00+00:00, run_start_date=2024-05-16 07:29:35.451113+00:00, run_end_date=2024-05-16 07:29:35.526474+00:00, run_duration=0.075361, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator[0m
[[34m2024-05-16 12:59:36,746[0m] {[34mdagrun.py:[0m530} ERROR[0m - Marking run <DagRun keypoints_dag @ 2024-05-15 00:00:00+00:00: scheduled__2024-05-15T00:00:00+00:00, externally triggered: False> failed[0m
[[34m2024-05-16 12:59:36,746[0m] {[34mdagrun.py:[0m590} INFO[0m - DagRun Finished: dag_id=keypoints_dag, execution_date=2024-05-15 00:00:00+00:00, run_id=scheduled__2024-05-15T00:00:00+00:00, run_start_date=2024-05-16 07:29:32.087198+00:00, run_end_date=2024-05-16 07:29:36.746734+00:00, run_duration=4.659536, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-15 00:00:00+00:00, data_interval_end=2024-05-16 00:00:00+00:00, dag_hash=d0134bdedf6caf352f8852ce87e68cb9[0m
[[34m2024-05-16 12:59:36,748[0m] {[34mdag.py:[0m2935} INFO[0m - Setting next_dagrun for keypoints_dag to 2024-05-16T00:00:00+00:00[0m
[[34m2024-05-16 13:03:40,109[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 13:04:59,144[0m] {[34mdag.py:[0m2935} INFO[0m - Setting next_dagrun for keypoints_dag to 2024-05-16T00:00:00+00:00[0m
[[34m2024-05-16 13:04:59,163[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-16 13:04:59,163[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-16 13:04:59,164[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-16 13:04:59,164[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-16 13:04:59,165[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='fetch_data', run_id='scheduled__2024-05-15T00:00:00+00:00', try_number=1) to executor with priority 2 and queue default[0m
[[34m2024-05-16 13:04:59,165[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'scheduled__2024-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:04:59,166[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'scheduled__2024-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:05:00,017[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BigDataLabProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-16 13:05:00,450[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-16 13:05:00,458[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-16 13:05:00,471[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-15T00:00:00+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-16 13:05:05,517[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.fetch_data run_id=scheduled__2024-05-15T00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2024-05-16 13:05:05,523[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=fetch_data, run_id=scheduled__2024-05-15T00:00:00+00:00, run_start_date=2024-05-16 07:35:00.507365+00:00, run_end_date=2024-05-16 07:35:05.323444+00:00, run_duration=4.816079, state=success, executor_state=success, try_number=1, max_tries=0, job_id=2, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator[0m
[[34m2024-05-16 13:05:06,403[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.preprocess_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-16 13:05:06,404[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-16 13:05:06,404[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-16 13:05:06,404[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.preprocess_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-16 13:05:06,405[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='preprocess_data', run_id='scheduled__2024-05-15T00:00:00+00:00', try_number=1) to executor with priority 1 and queue default[0m
[[34m2024-05-16 13:05:06,406[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'scheduled__2024-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:05:06,406[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'scheduled__2024-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:05:07,165[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BigDataLabProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-16 13:05:07,556[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-16 13:05:07,564[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-16 13:05:07,576[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.preprocess_data scheduled__2024-05-15T00:00:00+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-16 13:05:07,901[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.preprocess_data run_id=scheduled__2024-05-15T00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2024-05-16 13:05:07,906[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=preprocess_data, run_id=scheduled__2024-05-15T00:00:00+00:00, run_start_date=2024-05-16 07:35:07.611811+00:00, run_end_date=2024-05-16 07:35:07.690587+00:00, run_duration=0.078776, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator[0m
[[34m2024-05-16 13:05:08,720[0m] {[34mdagrun.py:[0m530} ERROR[0m - Marking run <DagRun keypoints_dag @ 2024-05-15 00:00:00+00:00: scheduled__2024-05-15T00:00:00+00:00, externally triggered: False> failed[0m
[[34m2024-05-16 13:05:08,720[0m] {[34mdagrun.py:[0m590} INFO[0m - DagRun Finished: dag_id=keypoints_dag, execution_date=2024-05-15 00:00:00+00:00, run_id=scheduled__2024-05-15T00:00:00+00:00, run_start_date=2024-05-16 07:34:59.149146+00:00, run_end_date=2024-05-16 07:35:08.720672+00:00, run_duration=9.571526, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-15 00:00:00+00:00, data_interval_end=2024-05-16 00:00:00+00:00, dag_hash=73569a8b120ddd0d069ac10eb63a8bed[0m
[[34m2024-05-16 13:05:08,722[0m] {[34mdag.py:[0m2935} INFO[0m - Setting next_dagrun for keypoints_dag to 2024-05-16T00:00:00+00:00[0m
[[34m2024-05-16 13:07:26,935[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.fetch_data manual__2024-05-16T07:37:25.386356+00:00 [scheduled]>[0m
[[34m2024-05-16 13:07:26,937[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-16 13:07:26,937[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-16 13:07:26,937[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.fetch_data manual__2024-05-16T07:37:25.386356+00:00 [scheduled]>[0m
[[34m2024-05-16 13:07:26,938[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='fetch_data', run_id='manual__2024-05-16T07:37:25.386356+00:00', try_number=1) to executor with priority 2 and queue default[0m
[[34m2024-05-16 13:07:26,938[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'manual__2024-05-16T07:37:25.386356+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:07:26,939[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'manual__2024-05-16T07:37:25.386356+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:07:27,715[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BigDataLabProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-16 13:07:28,148[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-16 13:07:28,156[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-16 13:07:28,169[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.fetch_data manual__2024-05-16T07:37:25.386356+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-16 13:07:33,381[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.fetch_data run_id=manual__2024-05-16T07:37:25.386356+00:00 exited with status success for try_number 1[0m
[[34m2024-05-16 13:07:33,386[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=fetch_data, run_id=manual__2024-05-16T07:37:25.386356+00:00, run_start_date=2024-05-16 07:37:28.436374+00:00, run_end_date=2024-05-16 07:37:33.206977+00:00, run_duration=4.770603, state=success, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator[0m
[[34m2024-05-16 13:07:34,243[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.preprocess_data manual__2024-05-16T07:37:25.386356+00:00 [scheduled]>[0m
[[34m2024-05-16 13:07:34,243[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-16 13:07:34,244[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-16 13:07:34,244[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.preprocess_data manual__2024-05-16T07:37:25.386356+00:00 [scheduled]>[0m
[[34m2024-05-16 13:07:34,245[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='preprocess_data', run_id='manual__2024-05-16T07:37:25.386356+00:00', try_number=1) to executor with priority 1 and queue default[0m
[[34m2024-05-16 13:07:34,245[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'manual__2024-05-16T07:37:25.386356+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:07:34,246[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'manual__2024-05-16T07:37:25.386356+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:07:35,031[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BigDataLabProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-16 13:07:35,380[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-16 13:07:35,388[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-16 13:07:35,400[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.preprocess_data manual__2024-05-16T07:37:25.386356+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-16 13:07:35,721[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.preprocess_data run_id=manual__2024-05-16T07:37:25.386356+00:00 exited with status success for try_number 1[0m
[[34m2024-05-16 13:07:35,727[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=preprocess_data, run_id=manual__2024-05-16T07:37:25.386356+00:00, run_start_date=2024-05-16 07:37:35.436225+00:00, run_end_date=2024-05-16 07:37:35.509834+00:00, run_duration=0.073609, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator[0m
[[34m2024-05-16 13:07:36,554[0m] {[34mdagrun.py:[0m530} ERROR[0m - Marking run <DagRun keypoints_dag @ 2024-05-16 07:37:25.386356+00:00: manual__2024-05-16T07:37:25.386356+00:00, externally triggered: True> failed[0m
[[34m2024-05-16 13:07:36,554[0m] {[34mdagrun.py:[0m590} INFO[0m - DagRun Finished: dag_id=keypoints_dag, execution_date=2024-05-16 07:37:25.386356+00:00, run_id=manual__2024-05-16T07:37:25.386356+00:00, run_start_date=2024-05-16 07:37:26.921254+00:00, run_end_date=2024-05-16 07:37:36.554935+00:00, run_duration=9.633681, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-05-15 00:00:00+00:00, data_interval_end=2024-05-16 00:00:00+00:00, dag_hash=73569a8b120ddd0d069ac10eb63a8bed[0m
[[34m2024-05-16 13:07:36,556[0m] {[34mdag.py:[0m2935} INFO[0m - Setting next_dagrun for keypoints_dag to 2024-05-16T00:00:00+00:00[0m
[[34m2024-05-16 13:08:40,941[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 13:13:42,003[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 13:18:42,279[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 13:23:43,087[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 13:27:51,233[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.fetch_data manual__2024-05-16T07:57:50.690130+00:00 [scheduled]>[0m
[[34m2024-05-16 13:27:51,235[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-16 13:27:51,235[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-16 13:27:51,236[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.fetch_data manual__2024-05-16T07:57:50.690130+00:00 [scheduled]>[0m
[[34m2024-05-16 13:27:51,237[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='fetch_data', run_id='manual__2024-05-16T07:57:50.690130+00:00', try_number=1) to executor with priority 3 and queue default[0m
[[34m2024-05-16 13:27:51,237[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'manual__2024-05-16T07:57:50.690130+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:27:51,238[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'manual__2024-05-16T07:57:50.690130+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:27:52,157[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BigDataLabProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-16 13:27:52,527[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-16 13:27:52,535[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-16 13:27:52,549[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.fetch_data manual__2024-05-16T07:57:50.690130+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-16 13:27:58,326[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.fetch_data run_id=manual__2024-05-16T07:57:50.690130+00:00 exited with status success for try_number 1[0m
[[34m2024-05-16 13:27:58,332[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=fetch_data, run_id=manual__2024-05-16T07:57:50.690130+00:00, run_start_date=2024-05-16 07:57:52.780464+00:00, run_end_date=2024-05-16 07:57:58.153461+00:00, run_duration=5.372997, state=success, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator[0m
[[34m2024-05-16 13:27:59,352[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.unzip_data manual__2024-05-16T07:57:50.690130+00:00 [scheduled]>[0m
[[34m2024-05-16 13:27:59,353[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-16 13:27:59,353[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-16 13:27:59,354[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.unzip_data manual__2024-05-16T07:57:50.690130+00:00 [scheduled]>[0m
[[34m2024-05-16 13:27:59,354[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='unzip_data', run_id='manual__2024-05-16T07:57:50.690130+00:00', try_number=1) to executor with priority 2 and queue default[0m
[[34m2024-05-16 13:27:59,355[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'unzip_data', 'manual__2024-05-16T07:57:50.690130+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:27:59,356[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'unzip_data', 'manual__2024-05-16T07:57:50.690130+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:28:00,134[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BigDataLabProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-16 13:28:00,504[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-16 13:28:00,512[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-16 13:28:00,524[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.unzip_data manual__2024-05-16T07:57:50.690130+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-16 13:28:01,472[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.unzip_data run_id=manual__2024-05-16T07:57:50.690130+00:00 exited with status success for try_number 1[0m
[[34m2024-05-16 13:28:01,478[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=unzip_data, run_id=manual__2024-05-16T07:57:50.690130+00:00, run_start_date=2024-05-16 07:58:00.560302+00:00, run_end_date=2024-05-16 07:58:01.306051+00:00, run_duration=0.745749, state=success, executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator[0m
[[34m2024-05-16 13:28:02,353[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.preprocess_data manual__2024-05-16T07:57:50.690130+00:00 [scheduled]>[0m
[[34m2024-05-16 13:28:02,353[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-16 13:28:02,354[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-16 13:28:02,354[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.preprocess_data manual__2024-05-16T07:57:50.690130+00:00 [scheduled]>[0m
[[34m2024-05-16 13:28:02,355[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='preprocess_data', run_id='manual__2024-05-16T07:57:50.690130+00:00', try_number=1) to executor with priority 1 and queue default[0m
[[34m2024-05-16 13:28:02,355[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'manual__2024-05-16T07:57:50.690130+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:28:02,356[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'manual__2024-05-16T07:57:50.690130+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:28:03,120[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BigDataLabProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-16 13:28:03,474[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-16 13:28:03,482[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-16 13:28:03,494[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.preprocess_data manual__2024-05-16T07:57:50.690130+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-16 13:28:11,785[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.preprocess_data run_id=manual__2024-05-16T07:57:50.690130+00:00 exited with status success for try_number 1[0m
[[34m2024-05-16 13:28:11,791[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=preprocess_data, run_id=manual__2024-05-16T07:57:50.690130+00:00, run_start_date=2024-05-16 07:58:03.530412+00:00, run_end_date=2024-05-16 07:58:11.546178+00:00, run_duration=8.015766, state=success, executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator[0m
[[34m2024-05-16 13:28:12,892[0m] {[34mdagrun.py:[0m545} INFO[0m - Marking run <DagRun keypoints_dag @ 2024-05-16 07:57:50.690130+00:00: manual__2024-05-16T07:57:50.690130+00:00, externally triggered: True> successful[0m
[[34m2024-05-16 13:28:12,892[0m] {[34mdagrun.py:[0m590} INFO[0m - DagRun Finished: dag_id=keypoints_dag, execution_date=2024-05-16 07:57:50.690130+00:00, run_id=manual__2024-05-16T07:57:50.690130+00:00, run_start_date=2024-05-16 07:57:51.216823+00:00, run_end_date=2024-05-16 07:58:12.892700+00:00, run_duration=21.675877, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-05-15 00:00:00+00:00, data_interval_end=2024-05-16 00:00:00+00:00, dag_hash=bac2574ba3bd249162c33f6c1b815ebf[0m
[[34m2024-05-16 13:28:12,894[0m] {[34mdag.py:[0m2935} INFO[0m - Setting next_dagrun for keypoints_dag to 2024-05-16T00:00:00+00:00[0m
[[34m2024-05-16 13:28:43,268[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 13:33:43,969[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
