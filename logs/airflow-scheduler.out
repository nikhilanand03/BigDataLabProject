[2024-05-16 12:53:40 +0530] [86627] [INFO] Starting gunicorn 20.1.0
[2024-05-16 12:53:40 +0530] [86627] [ERROR] Connection in use: ('0.0.0.0', 8793)
[2024-05-16 12:53:40 +0530] [86627] [ERROR] Retrying in 1 second.
[2024-05-16 12:53:41 +0530] [86627] [ERROR] Connection in use: ('0.0.0.0', 8793)
[2024-05-16 12:53:41 +0530] [86627] [ERROR] Retrying in 1 second.
[2024-05-16 12:53:42 +0530] [86627] [ERROR] Connection in use: ('0.0.0.0', 8793)
[2024-05-16 12:53:42 +0530] [86627] [ERROR] Retrying in 1 second.
[2024-05-16 12:53:43 +0530] [86627] [ERROR] Connection in use: ('0.0.0.0', 8793)
[2024-05-16 12:53:43 +0530] [86627] [ERROR] Retrying in 1 second.
[2024-05-16 12:53:44 +0530] [86627] [ERROR] Connection in use: ('0.0.0.0', 8793)
[2024-05-16 12:53:44 +0530][[34m2024-05-16 12:58:39,849[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 12:59:32,081[0m] {[34mdag.py:[0m2935} INFO[0m - Setting next_dagrun for keypoints_dag to 2024-05-16T00:00:00+00:00[0m
[[34m2024-05-16 12:59:32,105[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 2 tasks up for execution:
	<TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: keypoints_dag.preprocess_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-16 12:59:32,106[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 2 task instances ready to be queued[0m
[[34m2024-05-16 12:59:32,106[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-16 12:59:32,107[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 1/16 running and queued tasks[0m
[[34m2024-05-16 12:59:32,107[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>
	<TaskInstance: keypoints_dag.preprocess_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-16 12:59:32,108[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='fetch_data', run_id='scheduled__2024-05-15T00:00:00+00:00', try_number=1) to executor with priority 1 and queue default[0m
[[34m2024-05-16 12:59:32,108[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'scheduled__2024-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 12:59:32,109[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='preprocess_data', run_id='scheduled__2024-05-15T00:00:00+00:00', try_number=1) to executor with priority 1 and queue default[0m
[[34m2024-05-16 12:59:32,109[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'scheduled__2024-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 12:59:32,110[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'scheduled__2024-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 12:59:32,892[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BigDataLabProject/dags/preprocess_airflow.py[0m
[[34m2024-05-16 12:59:33,273[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-16 12:59:33,282[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-16 12:59:33,295[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-15T00:00:00+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-16 12:59:34,209[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'scheduled__2024-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 12:59:35,019[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BigDataLabProject/dags/preprocess_airflow.py[0m
[[34m2024-05-16 12:59:35,393[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-16 12:59:35,401[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-16 12:59:35,413[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.preprocess_data scheduled__2024-05-15T00:00:00+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-16 12:59:35,742[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.fetch_data run_id=scheduled__2024-05-15T00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2024-05-16 12:59:35,743[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.preprocess_data run_id=scheduled__2024-05-15T00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2024-05-16 12:59:35,748[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=fetch_data, run_id=scheduled__2024-05-15T00:00:00+00:00, run_start_date=2024-05-16 07:29:33.343244+00:00, run_end_date=2024-05-16 07:29:34.034137+00:00, run_duration=0.690893, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=2, pool=default_pool, queue=default, priority_weight=1, operator=BashOperator[0m
[[34m2024-05-16 12:59:35,748[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=preprocess_data, run_id=scheduled__2024-05-15T00:00:00+00:00, run_start_date=2024-05-16 07:29:35.451113+00:00, run_end_date=2024-05-16 07:29:35.526474+00:00, run_duration=0.075361, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator[0m
[[34m2024-05-16 12:59:36,746[0m] {[34mdagrun.py:[0m530} ERROR[0m - Marking run <DagRun keypoints_dag @ 2024-05-15 00:00:00+00:00: scheduled__2024-05-15T00:00:00+00:00, externally triggered: False> failed[0m
[[34m2024-05-16 12:59:36,746[0m] {[34mdagrun.py:[0m590} INFO[0m - DagRun Finished: dag_id=keypoints_dag, execution_date=2024-05-15 00:00:00+00:00, run_id=scheduled__2024-05-15T00:00:00+00:00, run_start_date=2024-05-16 07:29:32.087198+00:00, run_end_date=2024-05-16 07:29:36.746734+00:00, run_duration=4.659536, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-15 00:00:00+00:00, data_interval_end=2024-05-16 00:00:00+00:00, dag_hash=d0134bdedf6caf352f8852ce87e68cb9[0m
[[34m2024-05-16 12:59:36,748[0m] {[34mdag.py:[0m2935} INFO[0m - Setting next_dagrun for keypoints_dag to 2024-05-16T00:00:00+00:00[0m
[[34m2024-05-16 13:03:40,109[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 13:04:59,144[0m] {[34mdag.py:[0m2935} INFO[0m - Setting next_dagrun for keypoints_dag to 2024-05-16T00:00:00+00:00[0m
[[34m2024-05-16 13:04:59,163[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-16 13:04:59,163[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-16 13:04:59,164[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-16 13:04:59,164[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-16 13:04:59,165[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='fetch_data', run_id='scheduled__2024-05-15T00:00:00+00:00', try_number=1) to executor with priority 2 and queue default[0m
[[34m2024-05-16 13:04:59,165[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'scheduled__2024-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:04:59,166[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'scheduled__2024-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:05:00,017[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BigDataLabProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-16 13:05:00,450[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-16 13:05:00,458[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-16 13:05:00,471[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-15T00:00:00+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-16 13:05:05,517[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.fetch_data run_id=scheduled__2024-05-15T00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2024-05-16 13:05:05,523[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=fetch_data, run_id=scheduled__2024-05-15T00:00:00+00:00, run_start_date=2024-05-16 07:35:00.507365+00:00, run_end_date=2024-05-16 07:35:05.323444+00:00, run_duration=4.816079, state=success, executor_state=success, try_number=1, max_tries=0, job_id=2, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator[0m
[[34m2024-05-16 13:05:06,403[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.preprocess_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-16 13:05:06,404[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-16 13:05:06,404[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-16 13:05:06,404[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.preprocess_data scheduled__2024-05-15T00:00:00+00:00 [scheduled]>[0m
[[34m2024-05-16 13:05:06,405[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='preprocess_data', run_id='scheduled__2024-05-15T00:00:00+00:00', try_number=1) to executor with priority 1 and queue default[0m
[[34m2024-05-16 13:05:06,406[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'scheduled__2024-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:05:06,406[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'scheduled__2024-05-15T00:00:00+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:05:07,165[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BigDataLabProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-16 13:05:07,556[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-16 13:05:07,564[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-16 13:05:07,576[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.preprocess_data scheduled__2024-05-15T00:00:00+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-16 13:05:07,901[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.preprocess_data run_id=scheduled__2024-05-15T00:00:00+00:00 exited with status success for try_number 1[0m
[[34m2024-05-16 13:05:07,906[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=preprocess_data, run_id=scheduled__2024-05-15T00:00:00+00:00, run_start_date=2024-05-16 07:35:07.611811+00:00, run_end_date=2024-05-16 07:35:07.690587+00:00, run_duration=0.078776, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator[0m
[[34m2024-05-16 13:05:08,720[0m] {[34mdagrun.py:[0m530} ERROR[0m - Marking run <DagRun keypoints_dag @ 2024-05-15 00:00:00+00:00: scheduled__2024-05-15T00:00:00+00:00, externally triggered: False> failed[0m
[[34m2024-05-16 13:05:08,720[0m] {[34mdagrun.py:[0m590} INFO[0m - DagRun Finished: dag_id=keypoints_dag, execution_date=2024-05-15 00:00:00+00:00, run_id=scheduled__2024-05-15T00:00:00+00:00, run_start_date=2024-05-16 07:34:59.149146+00:00, run_end_date=2024-05-16 07:35:08.720672+00:00, run_duration=9.571526, state=failed, external_trigger=False, run_type=scheduled, data_interval_start=2024-05-15 00:00:00+00:00, data_interval_end=2024-05-16 00:00:00+00:00, dag_hash=73569a8b120ddd0d069ac10eb63a8bed[0m
[[34m2024-05-16 13:05:08,722[0m] {[34mdag.py:[0m2935} INFO[0m - Setting next_dagrun for keypoints_dag to 2024-05-16T00:00:00+00:00[0m
[[34m2024-05-16 13:07:26,935[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.fetch_data manual__2024-05-16T07:37:25.386356+00:00 [scheduled]>[0m
[[34m2024-05-16 13:07:26,937[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-16 13:07:26,937[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-16 13:07:26,937[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.fetch_data manual__2024-05-16T07:37:25.386356+00:00 [scheduled]>[0m
[[34m2024-05-16 13:07:26,938[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='fetch_data', run_id='manual__2024-05-16T07:37:25.386356+00:00', try_number=1) to executor with priority 2 and queue default[0m
[[34m2024-05-16 13:07:26,938[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'manual__2024-05-16T07:37:25.386356+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:07:26,939[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'manual__2024-05-16T07:37:25.386356+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:07:27,715[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BigDataLabProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-16 13:07:28,148[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-16 13:07:28,156[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-16 13:07:28,169[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.fetch_data manual__2024-05-16T07:37:25.386356+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-16 13:07:33,381[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.fetch_data run_id=manual__2024-05-16T07:37:25.386356+00:00 exited with status success for try_number 1[0m
[[34m2024-05-16 13:07:33,386[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=fetch_data, run_id=manual__2024-05-16T07:37:25.386356+00:00, run_start_date=2024-05-16 07:37:28.436374+00:00, run_end_date=2024-05-16 07:37:33.206977+00:00, run_duration=4.770603, state=success, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=2, operator=BashOperator[0m
[[34m2024-05-16 13:07:34,243[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.preprocess_data manual__2024-05-16T07:37:25.386356+00:00 [scheduled]>[0m
[[34m2024-05-16 13:07:34,243[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-16 13:07:34,244[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-16 13:07:34,244[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.preprocess_data manual__2024-05-16T07:37:25.386356+00:00 [scheduled]>[0m
[[34m2024-05-16 13:07:34,245[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='preprocess_data', run_id='manual__2024-05-16T07:37:25.386356+00:00', try_number=1) to executor with priority 1 and queue default[0m
[[34m2024-05-16 13:07:34,245[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'manual__2024-05-16T07:37:25.386356+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:07:34,246[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'manual__2024-05-16T07:37:25.386356+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:07:35,031[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BigDataLabProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-16 13:07:35,380[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-16 13:07:35,388[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-16 13:07:35,400[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.preprocess_data manual__2024-05-16T07:37:25.386356+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-16 13:07:35,721[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.preprocess_data run_id=manual__2024-05-16T07:37:25.386356+00:00 exited with status success for try_number 1[0m
[[34m2024-05-16 13:07:35,727[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=preprocess_data, run_id=manual__2024-05-16T07:37:25.386356+00:00, run_start_date=2024-05-16 07:37:35.436225+00:00, run_end_date=2024-05-16 07:37:35.509834+00:00, run_duration=0.073609, state=failed, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator[0m
[[34m2024-05-16 13:07:36,554[0m] {[34mdagrun.py:[0m530} ERROR[0m - Marking run <DagRun keypoints_dag @ 2024-05-16 07:37:25.386356+00:00: manual__2024-05-16T07:37:25.386356+00:00, externally triggered: True> failed[0m
[[34m2024-05-16 13:07:36,554[0m] {[34mdagrun.py:[0m590} INFO[0m - DagRun Finished: dag_id=keypoints_dag, execution_date=2024-05-16 07:37:25.386356+00:00, run_id=manual__2024-05-16T07:37:25.386356+00:00, run_start_date=2024-05-16 07:37:26.921254+00:00, run_end_date=2024-05-16 07:37:36.554935+00:00, run_duration=9.633681, state=failed, external_trigger=True, run_type=manual, data_interval_start=2024-05-15 00:00:00+00:00, data_interval_end=2024-05-16 00:00:00+00:00, dag_hash=73569a8b120ddd0d069ac10eb63a8bed[0m
[[34m2024-05-16 13:07:36,556[0m] {[34mdag.py:[0m2935} INFO[0m - Setting next_dagrun for keypoints_dag to 2024-05-16T00:00:00+00:00[0m
[[34m2024-05-16 13:08:40,941[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 13:13:42,003[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 13:18:42,279[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 13:23:43,087[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 13:27:51,233[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.fetch_data manual__2024-05-16T07:57:50.690130+00:00 [scheduled]>[0m
[[34m2024-05-16 13:27:51,235[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-16 13:27:51,235[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-16 13:27:51,236[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.fetch_data manual__2024-05-16T07:57:50.690130+00:00 [scheduled]>[0m
[[34m2024-05-16 13:27:51,237[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='fetch_data', run_id='manual__2024-05-16T07:57:50.690130+00:00', try_number=1) to executor with priority 3 and queue default[0m
[[34m2024-05-16 13:27:51,237[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'manual__2024-05-16T07:57:50.690130+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:27:51,238[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'manual__2024-05-16T07:57:50.690130+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:27:52,157[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BigDataLabProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-16 13:27:52,527[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-16 13:27:52,535[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-16 13:27:52,549[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.fetch_data manual__2024-05-16T07:57:50.690130+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-16 13:27:58,326[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.fetch_data run_id=manual__2024-05-16T07:57:50.690130+00:00 exited with status success for try_number 1[0m
[[34m2024-05-16 13:27:58,332[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=fetch_data, run_id=manual__2024-05-16T07:57:50.690130+00:00, run_start_date=2024-05-16 07:57:52.780464+00:00, run_end_date=2024-05-16 07:57:58.153461+00:00, run_duration=5.372997, state=success, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=3, operator=BashOperator[0m
[[34m2024-05-16 13:27:59,352[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.unzip_data manual__2024-05-16T07:57:50.690130+00:00 [scheduled]>[0m
[[34m2024-05-16 13:27:59,353[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-16 13:27:59,353[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-16 13:27:59,354[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.unzip_data manual__2024-05-16T07:57:50.690130+00:00 [scheduled]>[0m
[[34m2024-05-16 13:27:59,354[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='unzip_data', run_id='manual__2024-05-16T07:57:50.690130+00:00', try_number=1) to executor with priority 2 and queue default[0m
[[34m2024-05-16 13:27:59,355[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'unzip_data', 'manual__2024-05-16T07:57:50.690130+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:27:59,356[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'unzip_data', 'manual__2024-05-16T07:57:50.690130+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:28:00,134[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BigDataLabProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-16 13:28:00,504[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-16 13:28:00,512[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-16 13:28:00,524[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.unzip_data manual__2024-05-16T07:57:50.690130+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-16 13:28:01,472[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.unzip_data run_id=manual__2024-05-16T07:57:50.690130+00:00 exited with status success for try_number 1[0m
[[34m2024-05-16 13:28:01,478[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=unzip_data, run_id=manual__2024-05-16T07:57:50.690130+00:00, run_start_date=2024-05-16 07:58:00.560302+00:00, run_end_date=2024-05-16 07:58:01.306051+00:00, run_duration=0.745749, state=success, executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator[0m
[[34m2024-05-16 13:28:02,353[0m] {[34mscheduler_job.py:[0m288} INFO[0m - 1 tasks up for execution:
	<TaskInstance: keypoints_dag.preprocess_data manual__2024-05-16T07:57:50.690130+00:00 [scheduled]>[0m
[[34m2024-05-16 13:28:02,353[0m] {[34mscheduler_job.py:[0m317} INFO[0m - Figuring out tasks to run in Pool(name=default_pool) with 128 open slots and 1 task instances ready to be queued[0m
[[34m2024-05-16 13:28:02,354[0m] {[34mscheduler_job.py:[0m345} INFO[0m - DAG keypoints_dag has 0/16 running and queued tasks[0m
[[34m2024-05-16 13:28:02,354[0m] {[34mscheduler_job.py:[0m410} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: keypoints_dag.preprocess_data manual__2024-05-16T07:57:50.690130+00:00 [scheduled]>[0m
[[34m2024-05-16 13:28:02,355[0m] {[34mscheduler_job.py:[0m450} INFO[0m - Sending TaskInstanceKey(dag_id='keypoints_dag', task_id='preprocess_data', run_id='manual__2024-05-16T07:57:50.690130+00:00', try_number=1) to executor with priority 1 and queue default[0m
[[34m2024-05-16 13:28:02,355[0m] {[34mbase_executor.py:[0m82} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'manual__2024-05-16T07:57:50.690130+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:28:02,356[0m] {[34msequential_executor.py:[0m59} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'keypoints_dag', 'preprocess_data', 'manual__2024-05-16T07:57:50.690130+00:00', '--local', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py'][0m
[[34m2024-05-16 13:28:03,120[0m] {[34mdagbag.py:[0m500} INFO[0m - Filling up the DagBag from /Users/nikhilanand/BigDataLabProject/dags/preprocess_airflow.py[0m
curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"
[[34m2024-05-16 13:28:03,474[0m] {[34mexample_python_operator.py:[0m67} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2024-05-16 13:28:03,482[0m] {[34mtutorial_taskflow_api_etl_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_etl_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2024-05-16 13:28:03,494[0m] {[34mexample_kubernetes_executor.py:[0m38} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
Running <TaskInstance: keypoints_dag.preprocess_data manual__2024-05-16T07:57:50.690130+00:00 [queued]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[[34m2024-05-16 13:28:11,785[0m] {[34mscheduler_job.py:[0m504} INFO[0m - Executor reports execution of keypoints_dag.preprocess_data run_id=manual__2024-05-16T07:57:50.690130+00:00 exited with status success for try_number 1[0m
[[34m2024-05-16 13:28:11,791[0m] {[34mscheduler_job.py:[0m547} INFO[0m - TaskInstance Finished: dag_id=keypoints_dag, task_id=preprocess_data, run_id=manual__2024-05-16T07:57:50.690130+00:00, run_start_date=2024-05-16 07:58:03.530412+00:00, run_end_date=2024-05-16 07:58:11.546178+00:00, run_duration=8.015766, state=success, executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator[0m
[[34m2024-05-16 13:28:12,892[0m] {[34mdagrun.py:[0m545} INFO[0m - Marking run <DagRun keypoints_dag @ 2024-05-16 07:57:50.690130+00:00: manual__2024-05-16T07:57:50.690130+00:00, externally triggered: True> successful[0m
[[34m2024-05-16 13:28:12,892[0m] {[34mdagrun.py:[0m590} INFO[0m - DagRun Finished: dag_id=keypoints_dag, execution_date=2024-05-16 07:57:50.690130+00:00, run_id=manual__2024-05-16T07:57:50.690130+00:00, run_start_date=2024-05-16 07:57:51.216823+00:00, run_end_date=2024-05-16 07:58:12.892700+00:00, run_duration=21.675877, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-05-15 00:00:00+00:00, data_interval_end=2024-05-16 00:00:00+00:00, dag_hash=bac2574ba3bd249162c33f6c1b815ebf[0m
[[34m2024-05-16 13:28:12,894[0m] {[34mdag.py:[0m2935} INFO[0m - Setting next_dagrun for keypoints_dag to 2024-05-16T00:00:00+00:00[0m
[[34m2024-05-16 13:28:43,268[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 13:33:43,969[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 13:38:44,838[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 13:43:45,131[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 13:48:45,403[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 13:55:07,078[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 14:36:39,380[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 14:36:39,383[0m] {[34mscheduler_job.py:[0m1137} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2024-05-16 14:43:33,785[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 14:48:34,473[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 14:53:35,657[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 14:58:35,677[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 15:03:36,456[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 15:09:09,540[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 15:59:52,717[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 16:04:53,439[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 16:27:04,974[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 16:32:05,821[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 16:37:06,697[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 16:42:07,568[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 16:47:08,448[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 16:52:09,474[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 16:57:10,332[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 17:02:10,943[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 17:16:53,034[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 17:21:53,914[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 17:26:54,741[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 18:35:12,927[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 18:35:12,930[0m] {[34mscheduler_job.py:[0m1137} INFO[0m - Marked 1 SchedulerJob instances as failed[0m
[[34m2024-05-16 18:40:13,671[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 18:45:14,541[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 18:54:18,575[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 19:07:32,557[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 19:12:33,143[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 19:17:34,341[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 19:33:05,469[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 19:38:05,522[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 19:43:06,274[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 19:48:07,142[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 19:53:08,015[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 19:58:08,938[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 20:03:09,840[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 20:08:10,163[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 20:13:10,340[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 20:18:11,406[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 20:23:12,178[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 20:28:13,517[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 20:33:14,714[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 20:38:15,660[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 20:43:15,682[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 20:48:16,580[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 20:53:17,430[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 20:58:17,556[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 21:03:18,429[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 21:08:19,800[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 21:13:20,098[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 21:18:21,224[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 21:23:22,573[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 21:28:23,015[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 21:33:24,501[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 21:38:24,931[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 21:43:25,782[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 21:48:26,721[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 21:53:27,424[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 21:58:28,111[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 22:03:28,999[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 22:08:30,009[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 22:13:30,853[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 22:18:31,688[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 22:23:32,701[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 22:28:33,505[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 22:33:34,364[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 22:38:35,205[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 22:43:36,167[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 22:48:36,476[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 22:53:37,351[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 22:58:37,721[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 23:03:38,558[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 23:08:39,149[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 23:13:40,006[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 23:18:40,844[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 23:23:41,225[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 23:28:41,435[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 23:33:41,807[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 23:38:42,677[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 23:43:43,288[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 23:48:43,504[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 23:53:44,345[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-16 23:58:45,183[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 00:03:45,329[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 00:08:45,916[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 00:13:46,801[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 00:18:47,896[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 00:23:48,730[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 00:28:49,103[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 00:33:50,043[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 00:38:50,897[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 00:43:51,763[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 00:48:52,508[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 00:53:53,018[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 00:58:53,073[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 01:03:53,253[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 01:08:54,275[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 01:13:55,090[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 01:18:55,126[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 01:23:56,316[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 01:28:57,113[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 01:33:57,712[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 01:38:58,578[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 01:43:58,757[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 01:48:59,027[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 01:53:59,929[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 01:59:00,146[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 02:04:01,082[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 02:09:01,814[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 02:14:02,220[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 02:19:03,365[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 02:24:04,355[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 02:29:05,161[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 02:34:05,949[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 02:39:06,713[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 02:44:07,564[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 02:49:08,453[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 02:54:08,538[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 02:59:09,196[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 03:04:10,105[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 03:09:10,762[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 03:14:10,786[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 03:19:11,884[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 03:24:12,661[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 03:29:13,572[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 03:34:14,471[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 03:39:15,820[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 03:44:16,692[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 03:49:17,969[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 03:54:19,309[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 03:59:20,054[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
[[34m2024-05-17 04:04:20,745[0m] {[34mscheduler_job.py:[0m1114} INFO[0m - Resetting orphaned tasks for active dag runs[0m
