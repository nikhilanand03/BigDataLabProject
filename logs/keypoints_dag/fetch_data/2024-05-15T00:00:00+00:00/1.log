[2024-05-16 12:59:33,343] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-15T00:00:00+00:00 [queued]>
[2024-05-16 12:59:33,345] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-15T00:00:00+00:00 [queued]>
[2024-05-16 12:59:33,345] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-05-16 12:59:33,345] {taskinstance.py:1239} INFO - Starting attempt 1 of 1
[2024-05-16 12:59:33,345] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-05-16 12:59:33,349] {taskinstance.py:1259} INFO - Executing <Task(BashOperator): fetch_data> on 2024-05-15 00:00:00+00:00
[2024-05-16 12:59:33,359] {standard_task_runner.py:52} INFO - Started process 91146 to run task
[2024-05-16 12:59:33,363] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'scheduled__2024-05-15T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py', '--cfg-path', '/var/folders/hr/cn8gd6d15xv_6m22yptdtyhw0000gn/T/tmpo9psrqju', '--error-file', '/var/folders/hr/cn8gd6d15xv_6m22yptdtyhw0000gn/T/tmpxh_u_pz2']
[2024-05-16 12:59:33,365] {standard_task_runner.py:77} INFO - Job 2: Subtask fetch_data
[2024-05-16 12:59:33,387] {logging_mixin.py:109} INFO - Running <TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-15T00:00:00+00:00 [running]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[2024-05-16 12:59:33,405] {taskinstance.py:1424} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=keypoints_dag
AIRFLOW_CTX_TASK_ID=fetch_data
AIRFLOW_CTX_EXECUTION_DATE=2024-05-15T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-05-15T00:00:00+00:00
[2024-05-16 12:59:33,405] {subprocess.py:62} INFO - Tmp dir root location: 
 /var/folders/hr/cn8gd6d15xv_6m22yptdtyhw0000gn/T
[2024-05-16 12:59:33,406] {subprocess.py:74} INFO - Running command: ['bash', '-c', 'curl -o data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"']
[2024-05-16 12:59:33,417] {subprocess.py:85} INFO - Output:
[2024-05-16 12:59:33,501] {subprocess.py:89} INFO -   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
[2024-05-16 12:59:33,501] {subprocess.py:89} INFO -                                  Dload  Upload   Total   Spent    Left  Speed
[2024-05-16 12:59:34,022] {subprocess.py:89} INFO -   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0Warning: Failed to open the file data/training.csv: No such file or directory
[2024-05-16 12:59:34,023] {subprocess.py:89} INFO -   0 54.6M    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0
[2024-05-16 12:59:34,023] {subprocess.py:89} INFO - curl: (23) Failure writing output to destination
[2024-05-16 12:59:34,024] {subprocess.py:93} INFO - Command exited with return code 23
[2024-05-16 12:59:34,030] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/Users/nikhilanand/airflow_venv/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/Users/nikhilanand/airflow_venv/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/Users/nikhilanand/airflow_venv/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/Users/nikhilanand/airflow_venv/lib/python3.9/site-packages/airflow/operators/bash.py", line 187, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 23.
[2024-05-16 12:59:34,034] {taskinstance.py:1267} INFO - Marking task as FAILED. dag_id=keypoints_dag, task_id=fetch_data, execution_date=20240515T000000, start_date=20240516T072933, end_date=20240516T072934
[2024-05-16 12:59:34,039] {standard_task_runner.py:89} ERROR - Failed to execute job 2 for task fetch_data
Traceback (most recent call last):
  File "/Users/nikhilanand/airflow_venv/lib/python3.9/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/Users/nikhilanand/airflow_venv/lib/python3.9/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/Users/nikhilanand/airflow_venv/lib/python3.9/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/Users/nikhilanand/airflow_venv/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/Users/nikhilanand/airflow_venv/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/Users/nikhilanand/airflow_venv/lib/python3.9/site-packages/airflow/cli/commands/task_command.py", line 180, in _run_raw_task
    ti._run_raw_task(
  File "/Users/nikhilanand/airflow_venv/lib/python3.9/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/Users/nikhilanand/airflow_venv/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/Users/nikhilanand/airflow_venv/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/Users/nikhilanand/airflow_venv/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/Users/nikhilanand/airflow_venv/lib/python3.9/site-packages/airflow/operators/bash.py", line 187, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 23.
[2024-05-16 12:59:34,054] {local_task_job.py:154} INFO - Task exited with return code 1
[2024-05-16 12:59:34,062] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-05-16 13:05:00,507] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-15T00:00:00+00:00 [queued]>
[2024-05-16 13:05:00,509] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-15T00:00:00+00:00 [queued]>
[2024-05-16 13:05:00,509] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-05-16 13:05:00,509] {taskinstance.py:1239} INFO - Starting attempt 1 of 1
[2024-05-16 13:05:00,509] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-05-16 13:05:00,513] {taskinstance.py:1259} INFO - Executing <Task(BashOperator): fetch_data> on 2024-05-15 00:00:00+00:00
[2024-05-16 13:05:00,522] {standard_task_runner.py:52} INFO - Started process 95297 to run task
[2024-05-16 13:05:00,527] {standard_task_runner.py:76} INFO - Running: ['airflow', 'tasks', 'run', 'keypoints_dag', 'fetch_data', 'scheduled__2024-05-15T00:00:00+00:00', '--job-id', '2', '--raw', '--subdir', 'DAGS_FOLDER/preprocess_airflow.py', '--cfg-path', '/var/folders/hr/cn8gd6d15xv_6m22yptdtyhw0000gn/T/tmpqzaa_za6', '--error-file', '/var/folders/hr/cn8gd6d15xv_6m22yptdtyhw0000gn/T/tmpvfv39ojp']
[2024-05-16 13:05:00,528] {standard_task_runner.py:77} INFO - Job 2: Subtask fetch_data
[2024-05-16 13:05:00,549] {logging_mixin.py:109} INFO - Running <TaskInstance: keypoints_dag.fetch_data scheduled__2024-05-15T00:00:00+00:00 [running]> on host 1.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.0.ip6.arpa
[2024-05-16 13:05:00,565] {taskinstance.py:1424} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=airflow
AIRFLOW_CTX_DAG_ID=keypoints_dag
AIRFLOW_CTX_TASK_ID=fetch_data
AIRFLOW_CTX_EXECUTION_DATE=2024-05-15T00:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2024-05-15T00:00:00+00:00
[2024-05-16 13:05:00,565] {subprocess.py:62} INFO - Tmp dir root location: 
 /var/folders/hr/cn8gd6d15xv_6m22yptdtyhw0000gn/T
[2024-05-16 13:05:00,566] {subprocess.py:74} INFO - Running command: ['bash', '-c', 'curl -o /Users/nikhilanand/BigDataLabProject/data/training.csv "https://storage.googleapis.com/kaggle-competitions-data/kaggle-v2/62891/6855609/compressed/training.csv.zip?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1716100019&Signature=cz%2FG3wvyhW9QcUGYEAJ2bNXo6%2FIgCjqrbvxZGtyEC7%2BuoZkfBknQIlCuccOA82cjZjppreHd9XQcWXHNVvudXozi1iJJhcQzdZoqUzMux3lrBSZGGtZ%2BTYufw53k5WMQ2ByF%2Fm%2FhqJSjTlijkZOfRMJgbJkvGIcbMwIZxOwmXho%2FQjn6XGxSTsbNYaBfDl9%2FFtxqUz7eM42Y6WHzRbCbinHh0NqnRIAWNwh6ixYM7GRdSVhJiMmS7Ed9oLMvTUQBgi6qgyRB5BDejTJ6v31mCQdJcO0Nb53bbmzxl42nFMnBEtw3HhVgkD4rJcDbQXiWLRs0C1rKC5w2jGDA2c10sw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtraining.csv.zip"']
[2024-05-16 13:05:00,576] {subprocess.py:85} INFO - Output:
[2024-05-16 13:05:00,612] {subprocess.py:89} INFO -   % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
[2024-05-16 13:05:00,612] {subprocess.py:89} INFO -                                  Dload  Upload   Total   Spent    Left  Speed
[2024-05-16 13:05:05,301] {subprocess.py:89} INFO -   0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0  0 54.6M    0 64135    0     0  84882      0  0:11:14 --:--:--  0:11:14 84834  1 54.6M    1  894k    0     0   551k      0  0:01:41  0:00:01  0:01:40  551k 28 54.6M   28 15.4M    0     0  6027k      0  0:00:09  0:00:02  0:00:07 6025k 63 54.6M   63 34.7M    0     0  9834k      0  0:00:05  0:00:03  0:00:02 9833k 95 54.6M   95 51.9M    0     0  11.2M      0  0:00:04  0:00:04 --:--:-- 11.2M100 54.6M  100 54.6M    0     0  11.6M      0  0:00:04  0:00:04 --:--:-- 13.8M
[2024-05-16 13:05:05,305] {subprocess.py:93} INFO - Command exited with return code 0
[2024-05-16 13:05:05,323] {taskinstance.py:1267} INFO - Marking task as SUCCESS. dag_id=keypoints_dag, task_id=fetch_data, execution_date=20240515T000000, start_date=20240516T073500, end_date=20240516T073505
[2024-05-16 13:05:05,357] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-05-16 13:05:05,368] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
